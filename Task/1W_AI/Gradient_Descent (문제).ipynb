{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPhRpnGbllOL"
   },
   "source": [
    "# Assignment 1: Gradient Descent\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "본 과제에서는 경사하강법(Gradient Descent)를 구현해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rPsh3Gm07LV"
   },
   "source": [
    "## 1. Gradient Descent (1)\n",
    "\n",
    "먼저 [SymPy library](https://www.sympy.org/en/index.html)를 사용하여 간단한 이차함수의 최솟값을 gradient descent 방법으로 찾는 문제입니다. - AI Math 3강 참고\n",
    "\n",
    "(SymPy: 수학 방정식의 기호(symbol)를 사용하게 해 주는 라이브러리)\n",
    "\n",
    " #### **def func**\n",
    "- `sym.poly` 함수는 함수식을 정의해줍니다.\n",
    "\n",
    "- `sym.subs` 함수는 변수를 다른변수로 치환하거나 값을 대입해줍니다.\n",
    "\n",
    "\n",
    "#### **func_gradient**\n",
    "- `sym.diff` 함수는 도함수를 구해줍니다.\n",
    "- `func` 함수를 사용하여 미분값과 함수를 return하는 코드를 짜야합니다.\n",
    "\n",
    "#### **gradient_descent**\n",
    "- `init_point`는 경사하강법의 시작점을 의미합니다.\n",
    "- `lr_rate`는 learning rate로 step의 크기를 정해줍니다.\n",
    "- `epsilon`은 gradient 크기의 lower bound입니다.\n",
    "- init_point부터 경사하강법을 시작해서, 최소점이 출력될 수 있도록 코드를 짜야합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "4bnWEgmKuJ1J"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sympy as sym\n",
    "from sympy.abc import x\n",
    "from sympy.plotting import plot\n",
    "\n",
    "def func(val):\n",
    "    fun = sym.poly(x**2 + 2*x + 3)\n",
    "    return fun.subs(x, val), fun\n",
    "\n",
    "def func_gradient(fun, val):\n",
    "    ## TODO\n",
    "    _, function = func(val)\n",
    "    diff = sym.diff(function, x)\n",
    "    \n",
    "    return diff.subs(x, val), diff\n",
    "\n",
    "def gradient_descent(fun, init_point, lr_rate=1e-2, epsilon=1e-5):\n",
    "    cnt = 0\n",
    "    val = init_point\n",
    "    ## Todo\n",
    "    diff, _ = func_gradient(fun,val)\n",
    "    while(np.abs(diff) > epsilon):\n",
    "#         if(np.abs(diff) > 0):\n",
    "#             val -= lr_rate\n",
    "#         else:\n",
    "#             val += lr_rate\n",
    "\n",
    "        val = val - lr_rate*diff\n",
    "    \n",
    "        diff, _ = func_gradient(fun,val)\n",
    "        cnt += 1\n",
    "    print(\"함수: {}\\n연산횟수: {}\\n최소점: ({}, {})\".format(fun(val)[1], cnt, val, fun(val)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bROZY7uHCx3X"
   },
   "source": [
    "- 최종적으로 함수, 연산횟수, 최소점(-1,2)이 출력되면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "YFdjdR5vC0bG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "함수: Poly(x**2 + 2*x + 3, x, domain='ZZ')\n",
      "연산횟수: 788\n",
      "최소점: (-0.999995000443250, 2.00000000002500)\n"
     ]
    }
   ],
   "source": [
    "gradient_descent(fun = func, init_point = 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udMVpeMaXJhB"
   },
   "source": [
    "## 2. Gradient Descent (2)\n",
    "\n",
    "- 위의 예제에서 (-1,2)와 거의 동일한 최소점을 얻으셨나요? 그럼 이번에는 sympy library를 사용하지 않고 직접 Gradient Descent를 구현해봅시다!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1ygwhh4m2ex"
   },
   "source": [
    "$$ f'(x) = \\lim_{h \\rightarrow\n",
    " 0} \\frac{f(x+h)-f(x)}{h} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-nYj3r-Pm1Ln"
   },
   "source": [
    "- 원래의 미분 공식은 위와 같이 h를 0의 극한으로 보내면 되지만, 컴퓨터 상에서는 불가능하기 때문에 h에 1e-9와 같이 아주 작은 수를 넣어줌으로써 유사한 변화율을 구합니다.\n",
    "\n",
    "#### **difference_quotient(f, x, h)**\n",
    "- h만큼 움직였을 때의 변화율을 계산해주는 코드입니다.\n",
    "- h는 1e-9와 같이 매우 작은 수가 들어갑니다.\n",
    "- f에는 아래에 정의된 func 함수가 들어가고, x에는 변화율을 계산할 point가 들어갑니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "x4Lj4vl1KEDn"
   },
   "outputs": [],
   "source": [
    "def func(val):\n",
    "    fun = sym.poly(x**2 + 2*x + 3)\n",
    "    return fun.subs(x, val)\n",
    "\n",
    "def difference_quotient(f, x, h=1e-9):\n",
    "    ## Todo\n",
    "    result = (func(x + h) - func(x)) / h \n",
    "    \n",
    "    return result\n",
    "\n",
    "def gradient_descent(func, init_point, lr_rate=1e-2, epsilon=1e-5):\n",
    "    cnt = 0\n",
    "    val = init_point\n",
    "    ## Todo\n",
    "    diff = difference_quotient(func,val)\n",
    "    \n",
    "    while(np.abs(diff) > epsilon):\n",
    "        val = val - lr_rate*diff\n",
    "    \n",
    "        diff = difference_quotient(func,val)\n",
    "        cnt += 1\n",
    "    print(\"연산횟수: {}\\n최소점: ({}, {})\".format(cnt, val, func(val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2HzMqhLxC60r"
   },
   "source": [
    "- 최종적으로 연산횟수, 최소점(-1,2)이 출력되면 됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "rg_K54iaC-rS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "연산횟수: 672\n",
      "최소점: (-0.999994846459742, 2.00000000002656)\n"
     ]
    }
   ],
   "source": [
    "gradient_descent(func, init_point=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7C4-EzC4azr"
   },
   "source": [
    "## 3. Linear Regression\n",
    "- sympy library를 사용했을 때와 비슷한 결과값을 얻었나요?\n",
    "- 그럼 이제 본격적으로 Gradient Descent를 활용한 Linear Regression을 구현해 봅시다.\n",
    "\n",
    "### **3-1. Basic function**\n",
    "#### **Dataset 1** : train_x, train_y\n",
    "$$y = wx + b$$ $$y = 7x + 2$$\n",
    "- 위의 식을 알아내기 위해서 train_x, train_y의 data point를 가지고 Linear Regression을 진행합니다.\n",
    "\n",
    "#### **Gradient Descent**\n",
    "- error를 어떻게 계산하고, gradient를 통해 w와 b를 어떻게 조정해 나가는지에 대한 코드를 짜주시면 됩니다.\n",
    "    - error는 L2 norm으로 사용하고, np.sum 함수를 활용하시면 됩니다.\n",
    "- 결과값이 (7,2)와 유사하게 출력이 되면 성공입니다 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "Y4yOk4jR4aog"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w : 6.99263416114259 / b : 1.247922836317234 / error : 0.576933794621898\n"
     ]
    }
   ],
   "source": [
    "train_x = (np.random.rand(1000) - 0.5) * 10\n",
    "train_y = np.zeros_like(train_x)\n",
    "\n",
    "def func(val):\n",
    "    fun = sym.poly(7*x + 2)\n",
    "    return fun.subs(x, val)\n",
    "\n",
    "for i in range(1000):\n",
    "    train_y[i] = func(train_x[i])\n",
    "\n",
    "# initialize\n",
    "w, b = 0.0, 0.0\n",
    "\n",
    "lr_rate = 1e-2\n",
    "n_data = len(train_x)\n",
    "errors = []\n",
    "\n",
    "for i in range(100):\n",
    "    ## Todo\n",
    "    # 예측값 y\n",
    "    _y = w * train_x + b\n",
    "\n",
    "    # gradient\n",
    "    gradient_w = np.sum((_y - train_y) * train_x) / n_data\n",
    "    gradient_b = np.sum((_y - train_y)) / n_data\n",
    "\n",
    "    # w, b update with gradient and learning rate\n",
    "    w -= lr_rate * gradient_w\n",
    "    b -= lr_rate * gradient_b\n",
    "\n",
    "    # L2 norm과 np_sum 함수 활용해서 error 정의\n",
    "    error = np.sum((_y - train_y) ** 2) / n_data\n",
    "    # Error graph 출력하기 위한 부분\n",
    "    errors.append(error)\n",
    "\n",
    "print(\"w : {} / b : {} / error : {}\".format(w, b, error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqXgO77MDMvN"
   },
   "source": [
    "* 이제 plot 함수를 이용해서 full-batch gradient descent를 사용한 경우, error가 어떻게 줄어드는지 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "yPfF6tHbDQSR"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot(errors):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.ylabel('error')\n",
    "    plt.xlabel('time step')\n",
    "    plt.plot(errors)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "WmNczAWrDRm2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOU3Rq3TDVHp"
   },
   "source": [
    "### **3.2. More complicated function**\n",
    "#### **Dataset 2** : train_x, train_y\n",
    "- 이번에는 좀 더 복잡한 선형식에 대한 Regression을 진행해봅시다 !\n",
    "$$ y = w_0x_0 + w_1x_1 + w_2x_2 + b $$\n",
    "$$ y = x_0 + 3x_1 + 5x_2 + 7$$\n",
    "\n",
    "- 각 element의 계수를 beta_gd로 설정 : random initialize ( 목표 정답은 [1, 3, 5, 7] )\n",
    "- 이번에는 np.transpose 함수를 활용하여 gradient를 계산해봅시다 ( AI Math 4강 참고 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "ZbCGTSZt1LRC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After gradient descent, beta_gd : [1. 3. 5. 7.]\n"
     ]
    }
   ],
   "source": [
    "train_x = np.array([[1,1,1], [1,1,2], [1,2,2], [2,2,3], [2,3,3], [1,2,3]])\n",
    "train_y = np.dot(train_x, np.array([1,3,5])) + 7\n",
    "\n",
    "# random initialize\n",
    "beta_gd = [9.4, 10.6, -3.7, -1.2]\n",
    "# for constant element\n",
    "expand_x = np.array([np.append(x, [1]) for x in train_x])\n",
    "\n",
    "for t in range(50000):\n",
    "    ## Todo\n",
    "    pred_y = expand_x @ beta_gd\n",
    "    error = train_y - pred_y\n",
    "    \n",
    "    grad = -np.transpose(expand_x) @ error\n",
    "\n",
    "    beta_gd = beta_gd - 0.008 * grad\n",
    "    \n",
    "\n",
    "print(\"After gradient descent, beta_gd : {}\".format(beta_gd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHRiic4_6LlB"
   },
   "source": [
    "## 4. Stochastic Gradient Descent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YH4lCXrGDezY"
   },
   "source": [
    "- 3-1의 문제와 동일한 문제에 대해서 Stochastic Gradient Descent를 사용해봅시다.\n",
    "\n",
    "- Code와 dataset 모두 동일하게 사용하되, 기존의 Dataset으로부터 mini-batch를 구성해서 Gradient Descent를 진행해주시면 됩니다.\n",
    "    - mini-batch의 경우, **np.random.choice** 함수를 활용하셔서 1,000개의 dataset 중 10개를 뽑아서 만들어주시면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nn8133mq6obv"
   },
   "outputs": [],
   "source": [
    "train_x = (np.random.rand(1000) - 0.5) * 10\n",
    "train_y = np.zeros_like(train_x)\n",
    "\n",
    "def func(val):\n",
    "    fun = sym.poly(7*x + 2)\n",
    "    return fun.subs(x, val)\n",
    "\n",
    "for i in range(1000):\n",
    "    train_y[i] = func(train_x[i])\n",
    "\n",
    "# initialize\n",
    "w, b = 0.0, 0.0\n",
    "\n",
    "lr_rate = 1e-2\n",
    "n_data = 10\n",
    "errors = []\n",
    "\n",
    "for i in range(100):\n",
    "    ## Todo\n",
    "\n",
    "\n",
    "    # Error graph 출력하기 위한 부분\n",
    "    errors.append(error)\n",
    "\n",
    "print(\"w : {} / b : {} / error : {}\".format(w, b, error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ZE2CsZWDj7W"
   },
   "source": [
    "- plot 함수를 이용해서 mini-batch를 사용한 stochastic gradient descent의 경우, error가 어떻게 줄어드는지 확인해봅시다.\n",
    "- 3.1의 full-batch gradient descent를 사용한 경우와 plot을 통해 비교를 해보면 차이를 좀 더 명확히 확인할 수 있습니다.\n",
    "    - full-batch의 경우, 매 epoch마다 전체 dataset을 모두 사용하여 GD를 하기 때문에 그래프가 매끄럽지만, SGD에 비하여 초기 수렴속도가 느린 편입니다.\n",
    "    - mini-batch의 경우, 매 epoch마다 mini-batch를 sampling해서 GD를 하기 때문에 그래프가 매끄럽지 않지만, 그만큼 초기에 빠르게 minimum으로 수렴하는 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bCpqNbBsDidu"
   },
   "outputs": [],
   "source": [
    "plot(errors)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Gradient_Descent (문제).ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
